{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor and Add Operation\n",
    "\n",
    "ttnn.Tensor is the central type of ttnn.\n",
    "\n",
    "It is similar to torch.Tensor in the sense that it represents multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "The are a few key differences:\n",
    "\n",
    "- ttnn.Tensor can be stored in the SRAM or DRAM of TensTorrent devices\n",
    "- ttnn.Tensor doesn't have a concept of the strides, however it has a concept of row-major and tile layout\n",
    "- ttnn.Tensor has support for data types not supported by torch such as `bfp8` for example\n",
    "- ttnn.Tensor's shape stores the padding added to the tensor due to TILE_LAYOUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a tensor\n",
    "\n",
    "The recommended way to create a tensor is by using torch create function and then simply calling `ttnn.from_torch`. So, let's import both `torch` and `ttnn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 15:48:07.215 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/thienluu/.cache/ttnn,model_cache_path=/home/thienluu/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-08-21 15:48:07.277 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-08-21 15:48:07.279 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-08-21 15:48:07.282 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-08-21 15:48:07.286 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.reshape be migrated to C++?\n",
      "2024-08-21 15:48:07.286 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-08-21 15:48:07.287 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.squeeze be migrated to C++?\n",
      "2024-08-21 15:48:07.287 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-08-21 15:48:07.288 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-08-21 15:48:07.288 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.to_device be migrated to C++?\n",
      "2024-08-21 15:48:07.288 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.from_device be migrated to C++?\n",
      "2024-08-21 15:48:07.289 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-08-21 15:48:07.289 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-08-21 15:48:07.290 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-08-21 15:48:07.290 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.clone be migrated to C++?\n",
      "2024-08-21 15:48:07.291 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-08-21 15:48:07.291 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-08-21 15:48:07.291 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-08-21 15:48:07.292 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-08-21 15:48:07.295 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.matmul be migrated to C++?\n",
      "2024-08-21 15:48:07.295 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.linear be migrated to C++?\n",
      "2024-08-21 15:48:07.296 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.297 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d_legacy be migrated to C++?\n",
      "2024-08-21 15:48:07.297 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.global_avg_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.298 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.avg_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.302 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.matmul be migrated to C++?\n",
      "2024-08-21 15:48:07.302 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.linear be migrated to C++?\n",
      "2024-08-21 15:48:07.304 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-08-21 15:48:07.305 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.305 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d_legacy be migrated to C++?\n",
      "2024-08-21 15:48:07.306 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.global_avg_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.306 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.avg_pool2d be migrated to C++?\n",
      "2024-08-21 15:48:07.307 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Wormhole card (N150/N300), you will need to set the full Tensix available to be able to continue with this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ[\"WH_ARCH_YAML\"] = \"wormhole_b0_80_arch_eth_dispatch.yaml\"\n",
    "os.environ[\"GS_ARCH_YAML\"] = \"grayskull_120_arch.yaml\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create a torch Tensor and convert it to ttnn Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: ttnn.Shape([3, 4])\n",
      "layout: Layout.ROW_MAJOR\n",
      "dtype: DataType.FLOAT32\n"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.rand(3, 4)\n",
    "ttnn_tensor = ttnn.from_torch(torch_tensor)\n",
    "\n",
    "print(f\"shape: {ttnn_tensor.shape}\")\n",
    "print(f\"layout: {ttnn_tensor.layout}\")\n",
    "print(f\"dtype: {ttnn_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we get a tensor of shape [3, 4] in row-major layout with a data type of float32."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Host Storage: Borrowed vs Owned\n",
    "\n",
    "In this particular case, ttnn Tensor will borrow the data of the torch Tensor because ttnn Tensor is in row-major layout, torch tensor is contiguous and their data type matches.\n",
    "\n",
    "Let's print the current ttnn tensor, set element of torch tensor to 1234 and print the ttnn Tensor again to see borrowed storage in action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values:\n",
      "ttnn.Tensor([[ 0.14538,  0.60650,  ...,  0.07408,  0.95921],\n",
      "             [ 0.61710,  0.61210,  ...,  0.70045,  0.36176],\n",
      "             [ 0.42633,  0.62021,  ...,  0.52240,  0.18872]], shape=Shape([3, 4]), dtype=DataType::FLOAT32, layout=Layout::ROW_MAJOR)\n",
      "New values are all going to be 1234:\n",
      "ttnn.Tensor([[1234.00000, 1234.00000,  ..., 1234.00000, 1234.00000],\n",
      "             [1234.00000, 1234.00000,  ..., 1234.00000, 1234.00000],\n",
      "             [1234.00000, 1234.00000,  ..., 1234.00000, 1234.00000]], shape=Shape([3, 4]), dtype=DataType::FLOAT32, layout=Layout::ROW_MAJOR)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original values:\\n{ttnn_tensor}\")\n",
    "torch_tensor[:] = 1234\n",
    "print(f\"New values are all going to be 1234:\\n{ttnn_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try our best to use borrowed storage but if the torch data type is not supported in ttnn, then we don't have a choice but to automatically pick a different data type and copy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   0,    1,    2,  ...,   29,   30,   31],\n",
      "        [  32,   33,   34,  ...,   61,   62,   63],\n",
      "        [  64,   65,   66,  ...,   93,   94,   95],\n",
      "        ...,\n",
      "        [ 928,  929,  930,  ...,  957,  958,  959],\n",
      "        [ 960,  961,  962,  ...,  989,  990,  991],\n",
      "        [ 992,  993,  994,  ..., 1021, 1022, 1023]], dtype=torch.int32)\n",
      "\n",
      "tensor(35, dtype=torch.int32)\n",
      "\n",
      "input_rank =  2\n",
      "slice =  (1, 3)\n",
      "normalized slice =  (slice(None, 1, None), slice(None, 3, None))\n",
      "ttnn.Tensor([[    0,     1,     2]], shape=Shape([1, 3]), dtype=DataType::INT32, layout=Layout::ROW_MAJOR)\n",
      "\n",
      "input_rank =  2\n",
      "slice =  (slice(None, 1, None),)\n",
      "normalized slice =  (slice(None, 1, None),)\n",
      "ttnn.Tensor([[    0,     1,  ...,    46,    47],\n",
      "             [   64,    65,  ...,   110,   111],\n",
      "             ...,\n",
      "             [  912,   913,  ...,   958,   959],\n",
      "             [  976,   977,  ...,  1022,  1023]], shape=Shape([32, 32]), dtype=DataType::INT32, layout=Layout::TILE)\n"
     ]
    }
   ],
   "source": [
    "W = 32\n",
    "H = 32\n",
    "torch_tensor = torch.rand(W,H).to(torch.int32)\n",
    "for i in range(W):\n",
    "    for j in range(H):\n",
    "        torch_tensor[i][j] = i * H + j\n",
    "\n",
    "print(torch_tensor)\n",
    "ttnn_tensor = ttnn.from_torch(torch_tensor)\n",
    "# print(\"torch_tensor.dtype:\", torch_tensor.dtype)\n",
    "# print(\"ttnn_tensor.dtype:\", ttnn_tensor.dtype)\n",
    "# print(f\"Original values:\\n{ttnn_tensor}\")\n",
    "print() \n",
    "print(torch_tensor[1,3])\n",
    "\n",
    "print() \n",
    "print(ttnn_tensor[1,3])\n",
    "\n",
    "ttnn_tensor = ttnn.to_layout(ttnn_tensor, ttnn.TILE_LAYOUT)\n",
    "print()\n",
    "test_tensor = ttnn_tensor[1]\n",
    "print(ttnn_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Type\n",
    "\n",
    "The data type of the ttnn tensor can be controlled explicitly when conversion from torch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch_tensor.dtype: torch.float32\n",
      "ttnn_tensor.dtype: DataType.BFLOAT16\n",
      "__init__\n",
      "__doc__\n",
      "__module__\n",
      "shape\n",
      "dtype\n",
      "layout\n",
      "deallocate\n",
      "to\n",
      "track_ref_count\n",
      "sync\n",
      "extract_shard\n",
      "cpu\n",
      "cpu_sharded\n",
      "pad\n",
      "unpad\n",
      "pad_to_tile\n",
      "unpad_from_tile\n",
      "__repr__\n",
      "get_legacy_shape\n",
      "volume\n",
      "storage_type\n",
      "device\n",
      "devices\n",
      "to_torch\n",
      "to_numpy\n",
      "buffer\n",
      "buffer_address\n",
      "get_layout\n",
      "memory_config\n",
      "is_allocated\n",
      "is_contiguous\n",
      "is_sharded\n",
      "get_dtype\n",
      "shape_without_padding\n",
      "reshape\n",
      "tensor_id\n",
      "__matmul__\n",
      "__add__\n",
      "__radd__\n",
      "__sub__\n",
      "__mul__\n",
      "__rmul__\n",
      "__eq__\n",
      "__ne__\n",
      "__gt__\n",
      "__ge__\n",
      "__lt__\n",
      "__le__\n",
      "__getitem__\n",
      "__new__\n",
      "__hash__\n",
      "__str__\n",
      "__getattribute__\n",
      "__setattr__\n",
      "__delattr__\n",
      "__reduce_ex__\n",
      "__reduce__\n",
      "__subclasshook__\n",
      "__init_subclass__\n",
      "__format__\n",
      "__sizeof__\n",
      "__dir__\n",
      "__class__\n"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.rand(3, 4).to(torch.float32)\n",
    "ttnn_tensor = ttnn.from_torch(torch_tensor, dtype=ttnn.bfloat16)\n",
    "print(f\"torch_tensor.dtype: {torch_tensor.dtype}\")\n",
    "print(f\"ttnn_tensor.dtype: {ttnn_tensor.dtype}\")\n",
    "for att in ttnn_tensor.__dir__():\n",
    "    print(att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layout\n",
    "\n",
    "TensTorrent hardware is most efficiently utilized when running tensors using [tile layout](https://tenstorrent.github.io/ttnn/latest/ttnn/tensor.html#layout).\n",
    "The current tile size is hard-coded to [32, 32]. It was determined to be the optimal size for a tile given the compute, memory and data transfer constraints.\n",
    "\n",
    "\n",
    "ttnn provides easy and intuitive way to convert from row-major layout to tile layout and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m torch_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m4\u001b[39m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m      2\u001b[0m ttnn_tensor \u001b[38;5;241m=\u001b[39m ttnn\u001b[38;5;241m.\u001b[39mfrom_torch(torch_tensor)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensor in row-major layout:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mShape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mttnn_tensor\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLayout: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mttnn_tensor\u001b[38;5;241m.\u001b[39mlayout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mttnn_tensor\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.rand(3, 4).to(torch.float16)\n",
    "ttnn_tensor = ttnn.from_torch(torch_tensor)\n",
    "print(f\"Tensor in row-major layout:\\nShape {ttnn_tensor.shape}\\nLayout: {ttnn_tensor.layout}\\n{ttnn_tensor}\")\n",
    "ttnn_tensor = ttnn.to_layout(ttnn_tensor, ttnn.TILE_LAYOUT)\n",
    "print(f\"Tensor in tile layout:\\nShape {ttnn_tensor.shape}\\nLayout: {ttnn_tensor.layout}\\n{ttnn_tensor}\")\n",
    "ttnn_tensor = ttnn.to_layout(ttnn_tensor, ttnn.ROW_MAJOR_LAYOUT)\n",
    "print(f\"Tensor back in row-major layout:\\nShape {ttnn_tensor.shape}\\nLayout: {ttnn_tensor.layout}\\n{ttnn_tensor}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that padding is automatically inserted to put the tensor into tile layout and it automatically removed after the tensor is converted back to row-major layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conversion to tile layout can be done when caling `ttnn.from_torch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor in row-major layout:\n",
      "Shape ttnn.Shape([3[32], 4[32]]); Layout: Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "torch_tensor = torch.rand(3, 4).to(torch.float16)\n",
    "ttnn_tensor = ttnn.from_torch(torch_tensor, layout=ttnn.TILE_LAYOUT)\n",
    "print(f\"Tensor in row-major layout:\\nShape {ttnn_tensor.shape}; Layout: {ttnn_tensor.layout}\")\n",
    "ttnn_tensor = ttnn.to_layout(ttnn_tensor, ttnn.TILE_LAYOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `ttnn.to_torch` will always convert to row-major layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Device storage\n",
    "\n",
    "Finally, in order to actually utilize the tensor, we need to put it on the device. So, that we can run `ttnn` operations on it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open the device\n",
    "\n",
    "Use `ttnn.open` to get a handle to the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\u001b[32m2024-08-21 04:39:07.161\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.249\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.263\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.280\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.300\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.315\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.337\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:39:07.357\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Running with 1 cqs \n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1300 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | DPRINT enabled on device 0, all worker cores.\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | DPRINT Server attached device 0\n",
      "prefetcher_11: start\n",
      "dispatch_11: start\n"
     ]
    }
   ],
   "source": [
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tensors a and b with random values using torch\n",
    "\n",
    "To create a tensor that can be used by a `ttnn` operation:\n",
    "1. Create a tensor using torch\n",
    "2. Use `ttnn.from_torch` to convert the tensor from `torch.Tensor` to `ttnn.Tensor`, change the layout to `ttnn.TILE_LAYOUT` and put the tensor on the `device`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 1 page_size: 2048 dispatch_cb_page_size: 4096\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "torch_input_tensor_a = torch.rand((32, 32), dtype=torch.bfloat16)\n",
    "torch_input_tensor_b = torch.rand((32, 32), dtype=torch.bfloat16)\n",
    "\n",
    "input_tensor_a = ttnn.from_torch(torch_input_tensor_a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "input_tensor_b = ttnn.from_torch(torch_input_tensor_b, layout=ttnn.TILE_LAYOUT, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add tensor a and b\n",
    "\n",
    "`ttnn` supports operator overloading, therefore operator `+` can be used instead of `torch.add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 8 page_size: 2048 dispatch_cb_page_size: 4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write offset: 0 102240 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 156096 108 102240 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 272 272 167968 1 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 184352 1 32 \n"
     ]
    }
   ],
   "source": [
    "output_tensor = input_tensor_a + input_tensor_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the output tensor of the add in ttnn\n",
    "\n",
    "As can be seen the tensor of the same shape, layout and dtype is produced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: ttnn.Shape([32, 32])\n",
      "dtype: DataType.BFLOAT16\n",
      "layout: Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "print(f\"shape: {output_tensor.shape}\")\n",
    "print(f\"dtype: {output_tensor.dtype}\")\n",
    "print(f\"layout: {output_tensor.layout}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general we expect layout and dtype to stay the same when running most operations unless explicit arguments to modify them are passed in. However, there are obvious exceptions like an embedding operation that takes in `ttnn.uint32` and produces `ttnn.bfloat16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to torch and inspect the attributes of the torch tensor\n",
    "\n",
    "When converting the tensor to torch, `ttnn.to_torch` will move the tensor from the device, convert to tile layout and figure out the best data type to use on the torch side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: torch.Size([32, 32])\n",
      "dtype: torch.bfloat16\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 108\n",
      "cmd_write_linear_h_host\n",
      "process_write_host_h: 2064\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 108\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 16 16 200736 1 107408 \n",
      "cmd_write_linear_h_host\n",
      "process_write_host_h: 32\n"
     ]
    }
   ],
   "source": [
    "output_tensor = ttnn.to_torch(output_tensor)\n",
    "print(f\"shape: {output_tensor.shape}\")\n",
    "print(f\"dtype: {output_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the device\n",
    "\n",
    "Close the handle the device. This is a very important step as the device can hang currently if not closed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 108\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 16 16 213024 1 107408 \n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "cmd_write_linear_h_host\n",
      "process_write_host_h: 32\n",
      "dispatch terminate\n",
      "dispatch_11: out\n",
      "prefetcher_11: out\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | DPRINT Server dettached device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
