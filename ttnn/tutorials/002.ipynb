{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Multiplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're using a Wormhole card (N150/N300), you will need to set the full Tensix available to be able to continue with this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"WH_ARCH_YAML\"] = \"wormhole_b0_80_arch_eth_dispatch.yaml\"\n",
    "os.environ[\"GS_ARCH_YAML\"] = \"grayskull_120_arch.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-21 04:59:11.248 | DEBUG    | ttnn:<module>:82 - Initial ttnn.CONFIG:\n",
      "Config{cache_path=/home/thienluu/.cache/ttnn,model_cache_path=/home/thienluu/.cache/ttnn/models,tmp_dir=/tmp/ttnn,enable_model_cache=false,enable_fast_runtime_mode=true,throw_exception_on_fallback=false,enable_logging=false,enable_graph_report=false,enable_detailed_buffer_report=false,enable_detailed_tensor_report=false,enable_comparison_mode=false,comparison_mode_pcc=0.9999,root_report_path=generated/ttnn/reports,report_name=std::nullopt,std::nullopt}\n",
      "2024-08-21 04:59:11.313 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.pearson_correlation_coefficient be migrated to C++?\n",
      "2024-08-21 04:59:11.315 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.Conv1d be migrated to C++?\n",
      "2024-08-21 04:59:11.319 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-08-21 04:59:11.320 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.reshape be migrated to C++?\n",
      "2024-08-21 04:59:11.321 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.unsqueeze_to_4D be migrated to C++?\n",
      "2024-08-21 04:59:11.322 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.squeeze be migrated to C++?\n",
      "2024-08-21 04:59:11.323 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.from_torch be migrated to C++?\n",
      "2024-08-21 04:59:11.324 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.to_torch be migrated to C++?\n",
      "2024-08-21 04:59:11.325 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.to_device be migrated to C++?\n",
      "2024-08-21 04:59:11.325 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.from_device be migrated to C++?\n",
      "2024-08-21 04:59:11.326 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.allocate_tensor_on_device be migrated to C++?\n",
      "2024-08-21 04:59:11.326 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.copy_host_to_device_tensor be migrated to C++?\n",
      "2024-08-21 04:59:11.327 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.deallocate be migrated to C++?\n",
      "2024-08-21 04:59:11.328 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.clone be migrated to C++?\n",
      "2024-08-21 04:59:11.328 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.reallocate be migrated to C++?\n",
      "2024-08-21 04:59:11.329 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.load_tensor be migrated to C++?\n",
      "2024-08-21 04:59:11.329 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.dump_tensor be migrated to C++?\n",
      "2024-08-21 04:59:11.330 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.as_tensor be migrated to C++?\n",
      "2024-08-21 04:59:11.334 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.matmul be migrated to C++?\n",
      "2024-08-21 04:59:11.335 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.linear be migrated to C++?\n",
      "2024-08-21 04:59:11.336 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.337 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d_legacy be migrated to C++?\n",
      "2024-08-21 04:59:11.337 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.global_avg_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.338 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.avg_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.341 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.matmul be migrated to C++?\n",
      "2024-08-21 04:59:11.342 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.linear be migrated to C++?\n",
      "2024-08-21 04:59:11.344 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.conv2d be migrated to C++?\n",
      "2024-08-21 04:59:11.345 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.345 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.max_pool2d_legacy be migrated to C++?\n",
      "2024-08-21 04:59:11.346 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.global_avg_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.347 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.avg_pool2d be migrated to C++?\n",
      "2024-08-21 04:59:11.347 | WARNING  | ttnn.decorators:operation_decorator:790 - Should ttnn.Conv1d be migrated to C++?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                 Device\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Opening user mode device driver\n",
      "\u001b[32m2024-08-21 04:59:11.379\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.438\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.453\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.466\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.478\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.490\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.507\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[32m2024-08-21 04:59:11.518\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | \u001b[36mSiliconDriver  \u001b[0m - Detected 8 PCI devices : [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Initializing device 0. Program cache is NOT enabled\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Running with 1 cqs \n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | AI CLK for device 0 is:   1300 MHz\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | DPRINT enabled on device 0, all worker cores.\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | DPRINT Server attached device 0\n",
      "dispatch_11: start\n",
      "prefetcher_11: start\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ttnn\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "device_id = 0\n",
    "device = ttnn.open_device(device_id=device_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable program cache\n",
    "\n",
    "Enabling the program cache will speed up the execution of operations that run repeatedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Enabling program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.enable_program_cache(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1024\n",
    "k = 1024\n",
    "n = 1024"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize tensors a and b with random values using torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_a = torch.randn((m, k), dtype=torch.bfloat16)\n",
    "torch_b = torch.randn((k, n), dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n"
     ]
    }
   ],
   "source": [
    "a = ttnn.from_torch(torch_a, layout=ttnn.TILE_LAYOUT, device=device)\n",
    "b = ttnn.from_torch(torch_b, layout=ttnn.TILE_LAYOUT, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b\n",
    "The operation will run longer the first time because the kernels need to get compiled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: Function 'ttnn.matmul' executed in 0.4389 seconds\n",
      "4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 16 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 16 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 14 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "write offset: 0 102240 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 52 64 233504 1 102240 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 72 80 237600 1 102240 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 80 80 "
     ]
    }
   ],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-running the operation shows significant speed up by utilizing program caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "241696 1 102240 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 100 112 245792 1 102240 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 253984 1 107280 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 258080 1 107264 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 262176 1 107248 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'ttnn.matmul' executed in 0.0002 seconds\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 266272 1 107232 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 400 400 270368 1 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 319536 4 32 \n",
      "write offset: 0 102352 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 52 64 327712 1 102352 \n",
      "cmd_write_packed\n"
     ]
    }
   ],
   "source": [
    "output = a @ b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the layout of matrix multiplication output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dispatch_write_packed: 72 80 331808 1 102352 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 80 80 335904 1 102352 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 100 112 340000 1 102352 \n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "print(output.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, matrix multiplication produces outputs in a tile layout. That is because it's much more efficient to use this layout for computing matrix multiplications on TensTorrent accelerators compared to a row-major layout.\n",
    "\n",
    "And this is aslo why the logs show 2 tilize operations, as the inputs get automatically convered to the tile layout if they are in a row-major layout.\n",
    "\n",
    "Learn more about tile layout here: TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the result of the matrix multiplication\n",
    "\n",
    "To inspect the results we will first convert to row-major layout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing ttnn tensorcmd_write_packed\n",
      "dispatch_write_packed: 4 16 348192 1 107280 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 352288 1 107264 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 356384 1 107248 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 4 16 360480 1 107232 \n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 400 400 364576 1 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 413744 4 32 \n",
      "\n",
      "shape: ttnn.Shape([1024, 1024])\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 8\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 7 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "write offset: 0 102464 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 48 48 442512 32 102464 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 8\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 272 272 450592 2 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 475168 2 32 \n",
      "chunk of a tensor:\n",
      "ttnn.Tensor([[33.50000,  9.00000,  ..., -38.75000, -6.84375]], shape=Shape([1, 32]), dtype=DataType::BFLOAT16, layout=Layout::ROW_MAJOR)\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 40\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 4 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "write offset: 0 102512 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 116 128 500160 108 102512 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 40\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 16 16 520224 1 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 532512 1 32 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_linear_h_host\n",
      "process_write_host_h: 80\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 16 16 548896 1 107408 \n",
      "cmd_write_linear_h_host\n",
      "process_write_host_h: 32\n"
     ]
    }
   ],
   "source": [
    "output = ttnn.to_layout(output, ttnn.ROW_MAJOR_LAYOUT)\n",
    "\n",
    "print(\"Printing ttnn tensor\")\n",
    "print(f\"shape: {output.shape}\")\n",
    "print(f\"chunk of a tensor:\\n{output[:1, :32]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiply tensor a and b by using more performant config\n",
    "By default, matrix multiplication might not be as effecient as it could be. To speed it up further, the user can specify how many cores they want matrix multiplication to use. This can speed up the operation significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 16 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 63 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_write_paged is_dram: 0\n",
      "process_write_paged - pages: 16 page_size: 2048 dispatch_cb_page_size: 4096\n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_paged is_dram: 1\n",
      "process_write_paged - pages: 8 page_size: 2048 dispatch_cb_page_size: 4096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write offset: 0 102640 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 48 48 143504 32 102640 \n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      " DISPATCH WAIT 1a3b0 count 148\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 272 272 151584 2 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 176160 2 32 \n",
      "write offset: 0 102688 0\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 48 48 184464 32 102688 \n",
      "cmd_wait\n",
      " DISPATCH WAIT 1a3b0 count 180\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 272 272 192544 2 106592 \n",
      "cmd_write_packed_large\n",
      "cmd_wait\n",
      " DISPATCH BARRIER\n",
      "cmd_write_packed\n",
      "dispatch_write_packed: 36 48 217120 2 32 \n"
     ]
    }
   ],
   "source": [
    "a = ttnn.from_torch(torch_a)\n",
    "b = ttnn.from_torch(torch_b)\n",
    "\n",
    "a = ttnn.to_device(a, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "b = ttnn.to_device(b, device, memory_config=ttnn.L1_MEMORY_CONFIG)\n",
    "\n",
    "a = ttnn.to_layout(a, ttnn.TILE_LAYOUT)\n",
    "b = ttnn.to_layout(b, ttnn.TILE_LAYOUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run once to compile the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: ttnn.Shape([1024, 1024]), ttnn.Shape([1024, 1024])\n",
      "Input layout: Layout.TILE, Layout.TILE\n",
      "Output shape:  ttnn.Shape([1024, 1024])\n",
      "Output layout:  Layout.TILE\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input shape: {a.shape}, {b.shape}\")\n",
    "print(f\"Input layout: {a.layout}, {b.layout}\")\n",
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))\n",
    "print(\"Output shape: \", output.shape)\n",
    "print(\"Output layout: \", output.layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enjoy a massive speed up on the subsequent runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = ttnn.matmul(a, b, memory_config=ttnn.L1_MEMORY_CONFIG, core_grid=ttnn.CoreGrid(y=8, x=8))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Closing device 0\n",
      "\u001b[38;2;000;128;000m                  Metal\u001b[0m | \u001b[1m\u001b[38;2;100;149;237mINFO    \u001b[0m | Disabling and clearing program cache on device 0\n"
     ]
    }
   ],
   "source": [
    "ttnn.close_device(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
